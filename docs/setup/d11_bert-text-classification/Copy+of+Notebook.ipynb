{"cells":[{"cell_type":"markdown","metadata":{"id":"zGCJYkQj_Uu2"},"source":["<h2 align=center> Fine-Tune BERT for Text Classification with TensorFlow</h2>"]},{"cell_type":"markdown","metadata":{"id":"GTOWwSWGDDXs"},"source":["<div align=\"center\">\n","    <img width=\"512px\" src=\"https://drive.google.com/open?id=1Dwpp9IqzMygiePiF22QYmtckvsTrMtda\" />\n","    <img width=\"512px\" src=\"assets/download.png\" />\n","    <p style=\"text-align: center;color:gray\">Figure 1: BERT Classification Model</p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"4y2m1S6e12il"},"source":["<div align=\"center\">\n","    <img width=\"512px\" src='https://drive.google.com/uc?id=1fnJTeJs5HUpz7nix-F9E6EZdgUflqyEu' />\n","    <p style=\"text-align: center;color:gray\">Figure 1: BERT Classification Model</p>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"eYYYWqWr_WCC"},"source":["In this [project](https://www.coursera.org/projects/fine-tune-bert-tensorflow/), you will learn how to fine-tune a BERT model for text classification using TensorFlow and TF-Hub."]},{"cell_type":"markdown","metadata":{"id":"5yQG5PCO_WFx"},"source":["The pretrained BERT model used in this project is [available](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2) on [TensorFlow Hub](https://tfhub.dev/)."]},{"cell_type":"markdown","metadata":{"id":"7pKNS21u_WJo"},"source":["### Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"_3NHSMXv_WMv"},"source":["By the time you complete this project, you will be able to:\n","\n","- Build TensorFlow Input Pipelines for Text Data with the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API\n","- Tokenize and Preprocess Text for BERT\n","- Fine-tune BERT for text classification with TensorFlow 2 and [TF Hub](https://tfhub.dev)"]},{"cell_type":"markdown","metadata":{"id":"o6BEe-3-AVRQ"},"source":["### Prerequisites"]},{"cell_type":"markdown","metadata":{"id":"Sc9f-8rLAVUS"},"source":["In order to be successful with this project, it is assumed you are:\n","\n","- Competent in the Python programming language\n","- Familiar with deep learning for Natural Language Processing (NLP)\n","- Familiar with TensorFlow, and its Keras API"]},{"cell_type":"markdown","metadata":{"id":"MYXXV5n3Ab-4"},"source":["### Contents"]},{"cell_type":"markdown","metadata":{"id":"XhK-SYGyAjxe"},"source":["This project/notebook consists of several Tasks.\n","\n","- **[Task 1]()**: Introduction to the Project.\n","- **[Task 2]()**: Setup your TensorFlow and Colab Runtime\n","- **[Task 3]()**: Download and Import the Quora Insincere Questions Dataset\n","- **[Task 4]()**: Create tf.data.Datasets for Training and Evaluation\n","- **[Task 5]()**: Download a Pre-trained BERT Model from TensorFlow Hub\n","- **[Task 6]()**: Create a TensorFlow Input Pipeline with `tf.data`\n","- **[Task 7]()**: Add a Classification Head to the BERT `hub.KerasLayer`\n","- **[Task 8]()**: Fine-Tune BERT for Text Classification\n","- **[Task 9]()**: Evaluate the BERT Text Classification Model"]},{"cell_type":"markdown","metadata":{"id":"IaArqXjRAcBa"},"source":["## Task 2: Setup your TensorFlow and Colab Runtime."]},{"cell_type":"markdown","metadata":{"id":"GDDhjzZ5A4Q_"},"source":["You will only be able to use the Colab Notebook after you save it to your Google Drive folder. Click on the File menu and select “Save a copy in Drive…\n","\n","![Copy to Drive](https://drive.google.com/uc?id=1CH3eDmuJL8WR0AP1r3UE6sOPuqq8_Wl7)\n"]},{"cell_type":"markdown","metadata":{"id":"mpe6GhLuBJWB"},"source":["### Check GPU Availability\n","\n","Check if your Colab notebook is configured to use Graphical Processing Units (GPUs). If zero GPUs are available, check if the Colab notebook is configured to use GPUs (Menu > Runtime > Change Runtime Type).\n","\n","![Hardware Accelerator Settings](https://drive.google.com/uc?id=1qrihuuMtvzXJHiRV8M7RngbxFYipXKQx)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":680,"status":"ok","timestamp":1702762234264,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"8V9c8vzSL3aj","outputId":"711f2032-3573-4bb4-ab25-e4b84f9cd5cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Dec 16 21:30:33 2023       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"Obch3rAuBVf0"},"source":["### Install TensorFlow and TensorFlow Model Garden"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5995,"status":"ok","timestamp":1702762241579,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"bUQEY3dFB0jX","outputId":"d8c191c1-2782-40e5-b0ac-94b5d88d1b57"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.15.0\n"]}],"source":["import tensorflow as tf\n","print(tf.version.VERSION)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aU3YLZ1TYKUt"},"outputs":[],"source":["#!pip install -q tensorflow==2.3.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6054,"status":"ok","timestamp":1702762247607,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"AFRTC-zwUy6D","outputId":"7931e935-54bd-4313-c50f-5babd83fcc53"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'models'...\n","remote: Enumerating objects: 2650, done.\u001b[K\n","remote: Counting objects: 100% (2650/2650), done.\u001b[K\n","remote: Compressing objects: 100% (2311/2311), done.\u001b[K\n","remote: Total 2650 (delta 505), reused 1389 (delta 306), pack-reused 0\u001b[K\n","Receiving objects: 100% (2650/2650), 34.02 MiB | 13.18 MiB/s, done.\n","Resolving deltas: 100% (505/505), done.\n","Note: switching to '400d68abbccda2f0f6609e3a924467718b144233'.\n","\n","You are in 'detached HEAD' state. You can look around, make experimental\n","changes and commit them, and you can discard any commits you make in this\n","state without impacting any branches by switching back to a branch.\n","\n","If you want to create a new branch to retain commits you create, you may\n","do so (now or later) by using -c with the switch command. Example:\n","\n","  git switch -c <new-branch-name>\n","\n","Or undo this operation with:\n","\n","  git switch -\n","\n","Turn off this advice by setting config variable advice.detachedHead to false\n","\n","Updating files: 100% (2554/2554), done.\n"]}],"source":["!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":19721,"status":"ok","timestamp":1702764040750,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"jciUu3a1oey7","outputId":"6a63d9f4-af80-4e3f-e986-4ebb8f7ad982"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting fastapi\n","  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting kaleido\n","  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-multipart\n","  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn\n","  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas==1.5.3\n","  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pillow==8.3.2\n","  Downloading Pillow-8.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.3.post1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.26.2)\n","Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.7.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.13)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi)\n","  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Collecting h11>=0.8 (from uvicorn)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.6)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n","Installing collected packages: kaleido, typing-extensions, python-multipart, pillow, h11, uvicorn, starlette, pandas, fastapi\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 10.1.0\n","    Uninstalling Pillow-10.1.0:\n","      Successfully uninstalled Pillow-10.1.0\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.1.4\n","    Uninstalling pandas-2.1.4:\n","      Successfully uninstalled pandas-2.1.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.13 requires pillow>=9.0.0, but you have pillow 8.3.2 which is incompatible.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\n","torchvision 0.16.0+cu121 requires pillow!=8.3.*,>=5.3.0, but you have pillow 8.3.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed fastapi-0.105.0 h11-0.14.0 kaleido-0.2.1 pandas-1.5.3 pillow-8.3.2 python-multipart-0.0.6 starlette-0.27.0 typing-extensions-4.9.0 uvicorn-0.24.0.post1\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","pandas"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install fastapi kaleido python-multipart uvicorn pandas==1.5.3 pillow==8.3.2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26400,"status":"ok","timestamp":1702764149149,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"3H2G0571zLLs","outputId":"ef125155-ad96-4ef1-da44-29ef3b4ced2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.16.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.1.4 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\n","imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# install requirements to use tensorflow/models repository\n","!pip install -Uqr models/official/requirements.txt\n","# you may have to restart the runtime afterwards"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6034,"status":"ok","timestamp":1702762303295,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"0xH3jfnZMS5D","outputId":"fd9e9c0f-bd00-4717-f820-dda321e93c8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (0.15.0)\n","Requirement already satisfied: tensorflow<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.26.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (1.60.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.1)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15.0->tensorflow-text) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.5.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15.0->tensorflow-text) (3.2.2)\n"]}],"source":["!pip install tensorflow-text"]},{"cell_type":"markdown","metadata":{"id":"GVjksk4yCXur"},"source":["## Restart the Runtime\n","\n","**Note**\n","After installing the required Python packages, you'll need to restart the Colab Runtime Engine (Menu > Runtime > Restart runtime...)\n","\n","![Restart of the Colab Runtime Engine](https://drive.google.com/uc?id=1xnjAy2sxIymKhydkqb0RKzgVK9rh3teH)"]},{"cell_type":"markdown","metadata":{"id":"IMsEoT3Fg4Wg"},"source":["## Task 3: Download and Import the Quora Insincere Questions Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2317,"status":"ok","timestamp":1702762305594,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"GmqEylyFYTdP","outputId":"3b55dd20-134e-415b-bce3-e1f3adc01beb"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import sys\n","sys.path.append('models')\n","from official.nlp.data import classifier_data_lib\n","from official.nlp.bert import tokenization\n","from official.nlp import optimization\n","import tensorflow_text as text"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673,"status":"ok","timestamp":1702762306262,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"ZuX1lB8pPJ-W","outputId":"608dac35-86fd-4899-d0d8-f96be0f2ef89"},"outputs":[{"name":"stdout","output_type":"stream","text":["TF Version:  2.15.0\n","Eager mode:  True\n","Hub version:  0.15.0\n","GPU is available\n"]}],"source":["print(\"TF Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"Hub version: \", hub.__version__)\n","print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"]},{"cell_type":"markdown","metadata":{"id":"QtbwpWgyEZg7"},"source":["A downloadable copy of the [Quora Insincere Questions Classification data](https://www.kaggle.com/c/quora-insincere-questions-classification/data) can be found [https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip](https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip). Decompress and read the data into a pandas DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"elapsed":30,"status":"error","timestamp":1702762306263,"user":{"displayName":"Robin Mattern","userId":"11808501405776838568"},"user_tz":300},"id":"0nI-9itVwCCQ","outputId":"6e27f1aa-9359-40ee-cd8a-6ddee6773f1b"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-db0a34c5039d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_output\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_SetOutputMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m from .utils._tags import (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscovery\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreadpool_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m from .validation import (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreadpoolctl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    606\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 608\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_private\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_assert_valid_refcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gen_alignment_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextbuild\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mIS_PYSTON\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pyston_version_info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mHAS_REFCOUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getrefcount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mIS_PYSTON\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mHAS_LAPACK64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ilp64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0m_OLD_PROMOTION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_promotion_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'legacy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy.linalg._umath_linalg' has no attribute '_ilp64'"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeHE98KiMvDd"},"outputs":[],"source":["df = pd.read_csv('https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip',\n","                 compression = 'zip', low_memory = False)\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leRFRWJMocVa"},"outputs":[],"source":["df.tail(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqvN1X8Y-yAN"},"outputs":[],"source":["# target label distribution\n","df.target.plot(kind='hist', title=\"Target Distribution\")"]},{"cell_type":"markdown","metadata":{"id":"ELjswHcFHfp3"},"source":["## Task 4: Create tf.data.Datasets for Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"0eWAyiSCRQ4J"},"source":["We will be working with only a small portion of data since the data is huge and finetuning the bert model and that big size would considerabely take large amount of time."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fScULIGPwuWk"},"outputs":[],"source":["train_df, remaining = train_test_split(df, random_state = 42, train_size = 0.0075, stratify = df['target'])\n","val_df, _ = train_test_split(remaining, random_state=42, train_size = 0.00075, stratify = remaining['target'])\n","\n","print(train_df.shape, val_df.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQYMGT5_qLPX"},"outputs":[],"source":["with tf.device('/cpu:0'):\n","  train_data = tf.data.Dataset.from_tensor_slices((train_df['question_text'].values, train_df['target'].values))\n","  val_data = tf.data.Dataset.from_tensor_slices((val_df['question_text'].values, val_df['target'].values))\n","\n","  for text, label in train_data.take(1):\n","    print(text)\n","    print(label)"]},{"cell_type":"markdown","metadata":{"id":"e2-ReN88Hvy_"},"source":["## Task 5: Download a Pre-trained BERT Model from TensorFlow Hub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMb5M86b4-BU"},"outputs":[],"source":["\"\"\"\n","Each line of the dataset is composed of the review text and its label\n","- Data preprocessing consists of transforming text to BERT input features:\n","input_word_ids, input_mask, segment_ids\n","- In the process, tokenizing the text is done with the provided BERT model tokenizer\n","\"\"\"\n","\n","# Label categories\n","label_list = [0, 1]\n","\n","# maximum length of (token) input sequences\n","max_seq_length = 128  # default max length\n","\n","train_batch_size = 32\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBv-mBaOI8gs"},"outputs":[],"source":["# Get BERT layer and tokenizer\n","bert_layer = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4'\n","tokenizer = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e12SGZxZUXTC"},"outputs":[],"source":["# see an example of tokenization\n","bert_tokenizer = hub.load(tokenizer)\n","\n","bert_tokenizer.tokenize(['hi, how are you doing?'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEUezMK-zkkI"},"outputs":[],"source":["# create keras layers to preprocess the text and encode to fit to the bert model\n","\n","text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name = 'inputs')\n","\n","# preprocessing\n","preprocessor = hub.KerasLayer(tokenizer)\n","encoder_inputs = preprocessor(text_input)\n","\n","preprocess_model = tf.keras.Model(text_input, encoder_inputs)\n","\n","preprocess_model(tf.constant(['hi, how are you doing?']))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtAPWCqqO_t7"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dhdO6MjTbtn1"},"source":["## Task 8: Create a TensorFlow Input Pipeline with `tf.data`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHRdiO3dnPNr"},"outputs":[],"source":["with tf.device('/cpu:0'):\n","  # train\n","  train_data = train_data.shuffle(1000).batch(32, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n","\n","  # valid\n","  val_data = val_data.batch(32, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0Z2cy9GHQ8x"},"outputs":[],"source":["# train data spec\n","train_data.element_spec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGAH-ycYOmao"},"outputs":[],"source":["# valid data spec\n","val_data.element_spec"]},{"cell_type":"markdown","metadata":{"id":"GZxe-7yhPyQe"},"source":["## Task 9: Add a Classification Head to the BERT Layer"]},{"cell_type":"markdown","metadata":{"id":"9THH5V0Dw2HO"},"source":["<div align=\"center\">\n","    <img width=\"512px\" src='https://drive.google.com/uc?id=1fnJTeJs5HUpz7nix-F9E6EZdgUflqyEu' />\n","    <p style=\"text-align: center;color:gray\">Figure 3: BERT Layer</p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9il4gtlADcp"},"outputs":[],"source":["# Building the model\n","def create_model():\n","\n","  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name = 'inputs')\n","\n","  # preprocessing\n","  preprocessor = hub.KerasLayer(tokenizer)\n","  encoder_inputs = preprocessor(text_input)\n","\n","  # passing the encoded inputs to bert model\n","  encoder = hub.KerasLayer(bert_layer, trainable=True, name = 'BERT_Encoder')\n","  outputs = encoder(encoder_inputs)\n","\n","  # get the pooled outputs\n","  pooled_output = outputs['pooled_output']\n","\n","  # add output layer\n","  batch_norm = tf.keras.layers.BatchNormalization()(pooled_output)\n","  drop = tf.keras.layers.Dropout(0.4)(batch_norm)\n","  fc = tf.keras.layers.Dense(128, activation='relu')(drop)\n","  final_output = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(fc)\n","\n","  return tf.keras.Model(text_input, final_output)\n"]},{"cell_type":"markdown","metadata":{"id":"S6maM-vr7YaJ"},"source":["## Task 10: Fine-Tune BERT for Text Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptCtiiONsBgo"},"outputs":[],"source":["model = create_model()\n","model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5),\n","              loss = tf.keras.losses.BinaryCrossentropy(),\n","              metrics = [tf.keras.metrics.BinaryAccuracy()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6GJaFnkbMtPL"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcREcgPUHr9O"},"outputs":[],"source":["# Train model\n","epochs = 3\n","history = model.fit(train_data,\n","                    validation_data=val_data,\n","                    epochs=epochs,\n","                    verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"kNZl1lx_cA5Y"},"source":["## Task 11: Evaluate the BERT Text Classification Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dCjgrUYH_IsE"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_graphs(history, metric):\n","  plt.plot(history.history[metric])\n","  plt.plot(history.history['val_'+metric], '')\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(metric)\n","  plt.legend([metric, 'val_'+metric])\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6lrFRra_KmA"},"outputs":[],"source":["plot_graphs(history=history, metric='loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opu9neBA_98R"},"outputs":[],"source":["plot_graphs(history,'binary_accuracy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WquT1v6FbqQ6"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkhtCCgnUbY6"},"outputs":[],"source":["# testing on some samples\n","sample_example = [\"Do you have an adopted dog, how would you encourage people to adopt and not shop?\"]\n","model.predict(sample_example)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-wcRtQBHcuxR"},"outputs":[],"source":["sample_example = [\"HWhy do Europeans say they're the superior race, when in fact it took them over 2,000 years until mid 19th century to surpass China's largest economy?\"]\n","model.predict(sample_example)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiKuBGgfJUKv"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://github.com/abyanjan/Fine-Tune-BERT-for-Text-Classification/blob/master/Fine_Tune_BERT_for_Text_Classification_with_TensorFlow.ipynb","timestamp":1702759186696}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
